{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"12_선형회귀.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOHHvCZaqqty2lK2gW2zJoD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"mDpiAT5FxVPq"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"]},{"cell_type":"code","source":["# 학습 데이터 셋\n","x_train = torch.FloatTensor([[1], [2], [3], [4]])\n","y_train = torch.FloatTensor([[50], [70], [90], [85]])"],"metadata":{"id":"W-8IysbTxm3t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# y = W(weight)x + b(bias)\n","\n","'''\n","y = 20x + 15     <- 임의의 기울기\n","\n","시간     1   2   3   4\n","실제값  50  70  90  85\n","예측값  35  55  75  95\n","----------------------\n","오차값  15  15  15  -10\n","'''\n","\n","# MSE(평균 제곱 오차)\n","# 오차를 제곱하고 평균으로 나눈 것\n","\n","# 코드를 재실행해도 같은 랜덤 결과가 나옴\n","torch.manual_seed(10) # 숫자는 아무거나 주어도 상관 없음\n","\n","print(x_train)\n","print(x_train.shape)\n","\n","print(y_train)\n","print(y_train.shape)\n","\n","# 선형회귀의 핵심은 학습데이터와 가장 잘 맞는 직선을 찾는 작업\n","# requires_grad=True : 학습을 통해 변경되는 변수(값이 업데이트 가능하도록 변수로 설정)\n","W = torch.zeros(1, requires_grad=True)\n","print(W)\n","\n","b = torch.zeros(1, requires_grad=True)\n","print(b)\n","\n","H = x_train * W + b"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Q_C9k9CyOkL","executionInfo":{"status":"ok","timestamp":1661516971198,"user_tz":-540,"elapsed":13,"user":{"displayName":"Don kim","userId":"12731179959291392517"}},"outputId":"c4035475-300f-4e26-8f13-b4ed024f3c76"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1.],\n","        [2.],\n","        [3.],\n","        [4.]])\n","torch.Size([4, 1])\n","tensor([[50.],\n","        [70.],\n","        [90.],\n","        [85.]])\n","torch.Size([4, 1])\n","tensor([0.], requires_grad=True)\n","tensor([0.], requires_grad=True)\n"]}]},{"cell_type":"code","source":["# 비용 함수 선언(Cost Function, 손실함수(Loss Function), 오차 함수(Error Function))\n","cost = torch.mean((H - y_train) ** 2)\n","print(cost)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hD36F7cB0K5k","executionInfo":{"status":"ok","timestamp":1661516971199,"user_tz":-540,"elapsed":12,"user":{"displayName":"Don kim","userId":"12731179959291392517"}},"outputId":"adb7b692-132b-41a8-db83-6db6d96fd9be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(5681.2500, grad_fn=<MeanBackward0>)\n"]}]},{"cell_type":"code","source":["# optimizer\n","# 비용 함수의 값을 최소화 하는 W(기울기)와 b(절편)을 찾는 방법(알고리즘)\n","\n","# 경사 하강법(Gradient Descent)\n","# 가장 기본적인 최적화 알고리즘\n","# cost가 최소화 되는 지점은 접선의 기울기가 0이 되는 지점이며, 미분값이 0이 되는 지점\n","# 비용함수를 미분하여 현재 W에서의 접선의 기울기를 구하고 접선의 기울기가 낮은 방향으로 W의 값을 업데이트 하는 작업을 반복\n","\n","# SGD(Stochastic Gradient Descent)\n","# 배치 크기가 1인 경사하강법 알고리즘. \n","# 확률적 경사 하강법은 데이터 셋에서 무작위로 균일하게 선택한 하나의 샘플을 의존하여 각 단계의 예측 경사를 계산\n","\n","# 학습률(learning rate)\n","# 기울기의 값을 변경할 때 얼마나 크게 변경할지를 결정\n","optimizer= optim.SGD([W, b], lr=0.01)"],"metadata":{"id":"exF3R8wE1gWc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 에폭을 한번 돌 때마다 gradient를 0으로 초기화 해줘야 함\n","optimizer.zero_grad()\n","# 비용함수를 미분하여 gradient 계산\n","cost.backward() # 역전파\n","# W와 b를 업데이트\n","optimizer.step()"],"metadata":{"id":"CzvgSgZ02l1j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 에폭(epoch) : 전체 훈련 데이터가 학습에 한번 사용된 주기\n","# 총 에폭을 2000번. 100번마다 로그 출력\n","\n","x_train = torch.FloatTensor([[1], [2], [3], [4]]) # 시간\n","y_train = torch.FloatTensor([[50], [70], [90], [85]]) # 점수\n","\n","W = torch.zeros(1, requires_grad=True) # 변수 취급. 0으로 세팅\n","b = torch.zeros(1, requires_grad=True) # 변수 취급. 0으로 세팅\n","\n","optimizer = optim.SGD([W, b], lr=0.01) # SGD 최적화 사용.\n","\n","for epoch in range(1, 2001): # 2000바퀴\n","\n","    H = x_train * W + b\n","    cost = torch.mean((H - y_train) ** 2) # 오차값\n","\n","    optimizer.zero_grad() # 초기화\n","    cost.backward() # 역전파\n","    optimizer.step() # W, b 값을 업데이트\n","\n","    if epoch % 100 == 0:\n","        print('Epoch {:4d}/{} W:{:.3f} b:{:.3f} Cost:{:.6f}'.format(epoch, 2000, W.item(), b.item(), cost.item()))"],"metadata":{"id":"fdW8FPYE4nYi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661516971748,"user_tz":-540,"elapsed":559,"user":{"displayName":"Don kim","userId":"12731179959291392517"}},"outputId":"c54f5101-ec96-4308-d04a-caa000456dda"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch  100/2000 W:21.140 b:17.099 Cost:155.283264\n","Epoch  200/2000 W:18.901 b:23.679 Cost:106.389832\n","Epoch  300/2000 W:17.243 b:28.555 Cost:79.547897\n","Epoch  400/2000 W:16.014 b:32.168 Cost:64.812042\n","Epoch  500/2000 W:15.104 b:34.844 Cost:56.722210\n","Epoch  600/2000 W:14.429 b:36.828 Cost:52.280975\n","Epoch  700/2000 W:13.929 b:38.297 Cost:49.842865\n","Epoch  800/2000 W:13.559 b:39.386 Cost:48.504280\n","Epoch  900/2000 W:13.285 b:40.193 Cost:47.769455\n","Epoch 1000/2000 W:13.081 b:40.790 Cost:47.366051\n","Epoch 1100/2000 W:12.931 b:41.233 Cost:47.144569\n","Epoch 1200/2000 W:12.819 b:41.561 Cost:47.023003\n","Epoch 1300/2000 W:12.737 b:41.805 Cost:46.956245\n","Epoch 1400/2000 W:12.675 b:41.985 Cost:46.919601\n","Epoch 1500/2000 W:12.630 b:42.118 Cost:46.899479\n","Epoch 1600/2000 W:12.596 b:42.217 Cost:46.888451\n","Epoch 1700/2000 W:12.571 b:42.290 Cost:46.882359\n","Epoch 1800/2000 W:12.553 b:42.345 Cost:46.879055\n","Epoch 1900/2000 W:12.539 b:42.385 Cost:46.877216\n","Epoch 2000/2000 W:12.529 b:42.415 Cost:46.876221\n"]}]},{"cell_type":"code","source":["model = nn.Linear(1, 1) # 입력 데이터 수, 출력 데이터 수 -> 데이터 1개 넣어서 1개 결과 출력할 거다"],"metadata":{"id":"Y9v9eQ68pnS7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LinearRegressionModel(nn.Module): # nn.Module을 상속받음\n","  def __init__(self):\n","    super().__init__() # nn.Module의 부모 클래스를 불러옴\n","    self.linear = nn.Linear(1, 1) # __setattr__ 함수를 실행\n","  \n","  def forward(self, x): # forward()함수는 nn,Module 내부에 있는 함수여서 여기서 오버라이딩 해주는 것\n","    return self.linear(x) # x값 계산해서 결과값 리턴\n"],"metadata":{"id":"DOay0l5yrSjG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train = torch.FloatTensor([[1], [2], [3], [4]])\n","y_train = torch.FloatTensor([[50], [70], [90], [85]])"],"metadata":{"id":"y9kmzu0ns-xf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = LinearRegressionModel()"],"metadata":{"id":"IVGLruzytGOF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# optimizer는 model.parameter()를 통해 W와 b를 가져오고 러닝 레이트를 참고해서 가중치를 업데이트 하기 위한 객체\n","# model.parameters() : 입력 받고 레이어를 넘길 때의 가중치를 뽑아내는 메소드.\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"],"metadata":{"id":"s_UjaXSmtLNi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(1, 2001):\n","  H = model(x_train)\n","  cost = F.mse_loss(H, y_train)\n","\n","  optimizer.zero_grad() # 초기화\n","  cost.backward() # 역전파\n","  optimizer.step() # W, b 값을 업데이트\n","\n","  if epoch % 100 == 0:\n","        print('Epoch {:4d}/{} W:{:.3f} b:{:.3f} Cost:{:.6f}'.format(epoch, 2000, W.item(), b.item(), cost.item()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yd9R8dpytmmw","executionInfo":{"status":"ok","timestamp":1661516972485,"user_tz":-540,"elapsed":741,"user":{"displayName":"Don kim","userId":"12731179959291392517"}},"outputId":"602747b6-8685-4c2e-e113-fa9f50913fbc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch  100/2000 W:12.529 b:42.415 Cost:153.265686\n","Epoch  200/2000 W:12.529 b:42.415 Cost:105.282211\n","Epoch  300/2000 W:12.529 b:42.415 Cost:78.939957\n","Epoch  400/2000 W:12.529 b:42.415 Cost:64.478210\n","Epoch  500/2000 W:12.529 b:42.415 Cost:56.538986\n","Epoch  600/2000 W:12.529 b:42.415 Cost:52.180420\n","Epoch  700/2000 W:12.529 b:42.415 Cost:49.787621\n","Epoch  800/2000 W:12.529 b:42.415 Cost:48.473969\n","Epoch  900/2000 W:12.529 b:42.415 Cost:47.752815\n","Epoch 1000/2000 W:12.529 b:42.415 Cost:47.356934\n","Epoch 1100/2000 W:12.529 b:42.415 Cost:47.139538\n","Epoch 1200/2000 W:12.529 b:42.415 Cost:47.020203\n","Epoch 1300/2000 W:12.529 b:42.415 Cost:46.954765\n","Epoch 1400/2000 W:12.529 b:42.415 Cost:46.918800\n","Epoch 1500/2000 W:12.529 b:42.415 Cost:46.899036\n","Epoch 1600/2000 W:12.529 b:42.415 Cost:46.888199\n","Epoch 1700/2000 W:12.529 b:42.415 Cost:46.882233\n","Epoch 1800/2000 W:12.529 b:42.415 Cost:46.878956\n","Epoch 1900/2000 W:12.529 b:42.415 Cost:46.877151\n","Epoch 2000/2000 W:12.529 b:42.415 Cost:46.876186\n"]}]},{"cell_type":"code","source":["val = torch.FloatTensor([[5.0]])\n","pred = model(val)\n","print('학습 후 5시간 공부하면 예상되는 성적: ', pred)"],"metadata":{"id":"VYmPm2feuJuC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661517027236,"user_tz":-540,"elapsed":7,"user":{"displayName":"Don kim","userId":"12731179959291392517"}},"outputId":"261813a8-7bc6-4f5c-e111-55223976443c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["학습 후 5시간 공부하면 예상되는 성적:  tensor([[105.0592]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"code","source":["print(list(model.parameters()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j3tecXZCIdMN","executionInfo":{"status":"ok","timestamp":1661517062364,"user_tz":-540,"elapsed":4,"user":{"displayName":"Don kim","userId":"12731179959291392517"}},"outputId":"eb2ad76e-2b1b-41bf-8cf8-904fdd4bba62"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Parameter containing:\n","tensor([[12.5287]], requires_grad=True), Parameter containing:\n","tensor([42.4155], requires_grad=True)]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"quQjAqH6Il2b"},"execution_count":null,"outputs":[]}]}